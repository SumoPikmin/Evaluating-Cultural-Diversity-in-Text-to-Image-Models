## Evaluating Cultural Diversity in Text-to-Image Models (Bachelor Thesis Project)
T2I models have made impressive progress, but often fail to reflect cultural diversity. Existing benchmarks rely on simple, English-centric prompts, overlooking the complexity of people, places, and their interactions in diverse cultural contexts. To address this, we introduce a new benchmark for evaluating cultural knowledge in T2I models, featuring:

1) A scalable pipeline to extract and refine cultural concepts from Wikipedia using large language models (LLMs) and human validation.
2) A multilingual dataset of 2,500 cultural concepts spanning 4 diverse Geo-Cultures and 12 categories.
3) 37 modular prompt templates that generate up to 2.3 million complex, compositional text prompts when combined with our concept list.
4) A benchmark dataset of 48,000 images generated by three T2I models using both English and multilingual prompts.
5) A study comparing various metrics for quantifying cultural knowledge in generated images.
6) An analysis of three T2I models’ performance in generating culturally rich images in complex, dynamic contexts.

Our framework enables rigorous cultural evaluation by generating diverse and complex prompts from a naturally collected, varied concept list. We introduce a Visual Question Answering (VQA)-based metric to better assess text-image alignment, showing that prior metrics often fail to capture cultural understanding. Additionally, we highlight the limitations of current T2I models when handling complex, compositional cultural concepts and multilingual prompts.


### Running the Concept Collection Pipeline
Navigate to the `concept_collection_pipeline` directory
`cd concept_collection_pipeline`
To start the concept extraction process, submit the job file:
`sbatch concept_extraction.sh`
This will execute `concept_extraction.py` and generate the following output files in the `output/` directory:

1) `concept_candidates.csv` – a list of initial concept candidates for later refinement.
2) `images.csv` – extracted image–caption pairs from Wikipedia.

Next, run the filtering step:
This executes `concept_filtering.py`, which removes people and locations, then evaluates the remaining candidates using an LLM. Final concepts are selected based on manual assessment of the filtered candidates.

> Code will be released soon.

### Dataset
Our dataset includes cultural concepts across 4 Geo-Cultures (China, Germany, South Korea, Spain) and 12 categories (Beverages, Celebration, Clothing, Food, Fruit, Houses, Music (instruments), Religious Object, Religious Customs, Religious Settings, Sports, Utensils, Vegetables, Visual Arts).

The dataset is divided into two parts:

**A. Multilingual Cultural Concepts**  
A curated collection of 2,500 cultural concepts with examples such as:  

- **Chinese Dishes:** 北京烤鸭 (Peking Duck), 馒头 (Mantou), 饺子 (Jiaozi), 油条 (Youtiao)  
- **German Celebrations:** Oktoberfest, Weihnachtsmarkt (Christmas market), Maiwoche (May Week), Heinerfest  
- **South Korean Sports:** 합기도 (Hapkido), 씨름 (Ssireum), 태권도 (Taekwondo), 국궁 (Gakgung)  
- **Spanish Beverages:** Cortado, Horchata, Calimocho, Vermut (Vermouth)  

**B. Modular Benchmark**  
Contains 37 prompt templates combining a cultural concept, an entity, and a location — each selected from 5 unique options. Paired with the concept list, this enables generation of up to **2.3 million** diverse text prompts for evaluating compositional cultural knowledge in image generation. Examples include:

- *Beverages:* “Two friends are sitting in a popular restaurant, drinking 막걸리[^1] from South Korea. The image emphasizes 막걸리 as the main feature.”  
- *Clothing:* “Two co-workers are wearing a Tracht from Germany in the office. The image emphasizes Tracht as the main feature.”  
- *Food:* “At home, one person is cooking Paella from Spain. The image emphasizes Paella as the main feature.”  

**C. Generated Images**  
A set of 48,000 images created by three state-of-the-art T2I models — AltDiffusion-m9[^2], StableDiffusionXLbase-1.0[^3], and FLUX.1-dev[^4] — based on our concepts and prompt templates. Half of the images use English prompts, and half use multilingual prompts aligned with the cultural concepts.

> Dataset will be released soon.

---

[^1]: Makgeolli is a south korean alcoholic drink.
[^2]: https://huggingface.co/BAAI/AltDiffusion-m9
[^3]: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0
[^4]: https://github.com/black-forest-labs/flux


### Quantifying Cultural Knowledge 

We propose two criteria to evaluate cultural understanding in T2I models:

- **Concept Similarity:** How closely generated concepts match their real-world counterparts.  
- **Faithfulness:** How accurately the generated image reflects all parts of the text prompt.

We evaluate two automatic metrics:

- **CLIPScore** [Hessel et al., 2022] (https://arxiv.org/abs/2104.08718)
- **Visual Question Answering (VQA)**  ([Hu et al. 2023] (https://arxiv.org/abs/2303.11897), [Cho et al. 2024] (https://arxiv.org/abs/2310.18235), [Lin et al. 2024] (https://arxiv.org/abs/2404.01291))


In experiments with 200 aligned Wikipedia image–caption pairs, we find that embedding-based models (e.g., CLIP) fail to capture concept similarity reliably. In contrast, VQA models effectively identify culturally specific concepts in both real and synthetic images, making them a strong candidate for evaluating cultural alignment.

> Code and results for this section will be released soon.

---

### Evaluating Cultural Knowledge in T2I Models

We test three T2I models—AltDiffusion-m9, StableDiffusionXLbase-1.0, and FLUX.1-dev—on their understanding of complex cultural prompts via two experiments:

- **Monolingual:** Using prompt templates with cultural concepts to evaluate cross-cultural knowledge and generation quality.  
- **Multilingual:** Translating prompts into Mandarin, German, Korean, and Spanish to assess multilingual ability and cross-lingual consistency [Qi et al., 2023] (https://arxiv.org/abs/2310.10378).

>Code and results for this section will be released soon.


## Citation

This repository contains data/code derived from the thesis:

Kai Li, *Evaluating Cultural Diversity in Text-to-Image Models*,  
B.Sc. Computer Science, TU Darmstadt, 2024.

If you use this data/code, please cite the thesis as:

> Kai Li (2024). *Evaluating Cultural Diversity in Text-to-Image Models*.  
> B.Sc. Thesis, TU Darmstadt.

## License & Usage

The data/code in this repository is based on work from TU Darmstadt. Use is permitted for non-commercial research and educational purposes only. For other uses, please contact kaili.2001@hotmail.de.

